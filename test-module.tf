# ▼▼▼ Automatically generated by Terraform SubmoduleToool, PLEASE DON'T EDIT MANUALLY :) ▼▼▼
locals {
  consul_sync = {
    atomic                 = coalesce(var.consul_sync.atomic, true)
    chart                  = coalesce(var.consul_sync.chart, "consul")
    chart_version          = coalesce(var.consul_sync.chart_version, "0.39.0")
    cleanup_on_fail        = coalesce(var.consul_sync.cleanup_on_fail, true)
    consul_datacenter      = coalesce(var.consul_sync.consul_datacenter, "dev-euw1-general")
    consul_image           = coalesce(var.consul_sync.consul_image, "consul:1.11.0")
    consul_servers_address = coalesce(var.consul_sync.consul_servers_address, "dev-euw1-general.consul.appsflyer.platform")
    create_consul_sync     = coalesce(var.consul_sync.create_consul_sync, true)
    create_namespace       = coalesce(var.consul_sync.create_namespace, true)
    name                   = coalesce(var.consul_sync.name, "consul")
    namespace              = coalesce(var.consul_sync.namespace, "consul")
    repository             = coalesce(var.consul_sync.repository, "https://helm.releases.hashicorp.com")
    timeout                = coalesce(var.consul_sync.timeout, 300)
    wait                   = coalesce(var.consul_sync.wait, true)
    wait_for_jobs          = coalesce(var.consul_sync.wait_for_jobs, true)
    watch_any_namespace    = coalesce(var.consul_sync.watch_any_namespace, true)

    resources = var.consul_sync.resources != null ? merge(var.consul_sync.resources,
      tomap({
        limit_cpu     = contains(keys(var.consul_sync.resources), limit_cpu) != false ? var.consul_sync.resources.limit_cpu : "2"
        limit_memory  = contains(keys(var.consul_sync.resources), limit_memory) != false ? var.consul_sync.resources.limit_memory : "2Gi"
        requested_cpu = contains(keys(var.consul_sync.resources), requested_cpu) != false ? var.consul_sync.resources.requested_cpu : "0.5"
        requested_memory = contains(keys(var.consul_sync.resources), requested_memory) != false ? var.consul_sync.resources.requested_memory : "512Mi" }
      )) : {
      limit_cpu        = "2"
      limit_memory     = "2Gi"
      requested_cpu    = "0.5"
      requested_memory = "512Mi"
    }

  }

  cruise_control = {
    default_goals = coalesce(var.cruise_control.default_goals, [
      "com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal"
    ])
    hard_goals = coalesce(var.cruise_control.hard_goals, [
      "com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal"
    ])
    is_enabled     = coalesce(var.cruise_control.is_enabled, true)
    root_log_level = coalesce(var.cruise_control.root_log_level, "INFO")

    broker_capacity = var.cruise_control.broker_capacity != null ? merge(var.cruise_control.broker_capacity,
      tomap({
        cpu_utilization = contains(keys(var.cruise_control.broker_capacity), cpu_utilization) != false ? var.cruise_control.broker_capacity.cpu_utilization : 100
        inbound_network = contains(keys(var.cruise_control.broker_capacity), inbound_network) != false ? var.cruise_control.broker_capacity.inbound_network : "10000KB/s"
        outbound_network = contains(keys(var.cruise_control.broker_capacity), outbound_network) != false ? var.cruise_control.broker_capacity.outbound_network : "10000KB/s" }
      )) : {
      cpu_utilization  = 100
      inbound_network  = "10000KB/s"
      outbound_network = "10000KB/s"
    }
    jvm = var.cruise_control.jvm != null ? merge(var.cruise_control.jvm,
      tomap({
        xms = contains(keys(var.cruise_control.jvm), xms) != false ? var.cruise_control.jvm.xms : "512m"
        xmx = contains(keys(var.cruise_control.jvm), xmx) != false ? var.cruise_control.jvm.xmx : "2048m" }
      )) : {
      xms = "512m"
      xmx = "2048m"
    }
    resources = var.cruise_control.resources != null ? merge(var.cruise_control.resources,
      tomap({
        limit_cpu     = contains(keys(var.cruise_control.resources), limit_cpu) != false ? var.cruise_control.resources.limit_cpu : "2"
        limit_memory  = contains(keys(var.cruise_control.resources), limit_memory) != false ? var.cruise_control.resources.limit_memory : "2Gi"
        requested_cpu = contains(keys(var.cruise_control.resources), requested_cpu) != false ? var.cruise_control.resources.requested_cpu : "0.5"
        requested_memory = contains(keys(var.cruise_control.resources), requested_memory) != false ? var.cruise_control.resources.requested_memory : "512Mi" }
      )) : {
      limit_cpu        = "2"
      limit_memory     = "2Gi"
      requested_cpu    = "0.5"
      requested_memory = "512Mi"
    }

    global_config = var.cruise_control.global_config != null ? merge(var.cruise_control.global_config,
      tomap({
        "cpu.balance.threshold"             = contains(keys(var.cruise_control.global_config), "cpu.balance.threshold") != false ? var.cruise_control.global_config["cpu.balance.threshold"] : "1.1"
        "metadata.max.age.ms"               = contains(keys(var.cruise_control.global_config), "metadata.max.age.ms") != false ? var.cruise_control.global_config["metadata.max.age.ms"] : "60000"
        "send.buffer.bytes"                 = contains(keys(var.cruise_control.global_config), "send.buffer.bytes") != false ? var.cruise_control.global_config["send.buffer.bytes"] : "131072"
        "webserver.http.cors.enabled"       = contains(keys(var.cruise_control.global_config), "webserver.http.cors.enabled") != false ? var.cruise_control.global_config["webserver.http.cors.enabled"] : "true",
        "webserver.http.cors.exposeheaders" = contains(keys(var.cruise_control.global_config), "webserver.http.cors.exposeheaders") != false ? var.cruise_control.global_config["webserver.http.cors.exposeheaders"] : "User-Task-ID,Content-Type"
        "webserver.http.cors.origin"        = contains(keys(var.cruise_control.global_config), "webserver.http.cors.origin") != false ? var.cruise_control.global_config["webserver.http.cors.origin"] : "'*'",
        "webserver.security.enable"         = contains(keys(var.cruise_control.global_config), "webserver.security.enable") != false ? var.cruise_control.global_config["webserver.security.enable"] : "false"
        "webserver.ssl.enable" = contains(keys(var.cruise_control.global_config), "webserver.ssl.enable") != false ? var.cruise_control.global_config["webserver.ssl.enable"] : "false" }
      )) : {
      "cpu.balance.threshold"             = "1.1"
      "metadata.max.age.ms"               = "60000"
      "send.buffer.bytes"                 = "131072"
      "webserver.http.cors.enabled"       = "true",
      "webserver.http.cors.exposeheaders" = "User-Task-ID,Content-Type"
      "webserver.http.cors.origin"        = "'*'",
      "webserver.security.enable"         = "false"
      "webserver.ssl.enable"              = "false"
    }
    service_annotations = var.cruise_control.service_annotations != null ? merge(var.cruise_control.service_annotations,
      tomap({
        "consul.hashicorp.com/service-sync" = contains(keys(var.cruise_control.service_annotations), "consul.hashicorp.com/service-sync") != false ? var.cruise_control.service_annotations["consul.hashicorp.com/service-sync"] : "true" }
      )) : {
      "consul.hashicorp.com/service-sync" = "true"
    }
  }

  entity_operator = {
    reconciliation_interval_seconds = coalesce(var.entity_operator.reconciliation_interval_seconds, 120)
    root_log_level                  = coalesce(var.entity_operator.root_log_level, "INFO")

    resources = var.entity_operator.resources != null ? merge(var.entity_operator.resources,
      tomap({
        limit_cpu     = contains(keys(var.entity_operator.resources), limit_cpu) != false ? var.entity_operator.resources.limit_cpu : "1"
        limit_memory  = contains(keys(var.entity_operator.resources), limit_memory) != false ? var.entity_operator.resources.limit_memory : "512Mi"
        requested_cpu = contains(keys(var.entity_operator.resources), requested_cpu) != false ? var.entity_operator.resources.requested_cpu : "0.5"
        requested_memory = contains(keys(var.entity_operator.resources), requested_memory) != false ? var.entity_operator.resources.requested_memory : "256Mi" }
      )) : {
      limit_cpu        = "1"
      limit_memory     = "512Mi"
      requested_cpu    = "0.5"
      requested_memory = "256Mi"
    }

  }

  kafka = {
    bootstrap_node_port = coalesce(var.kafka.bootstrap_node_port, 32300)
    kafka_version       = coalesce(var.kafka.kafka_version, "3.0.0")
    replicas            = coalesce(var.kafka.replicas, 3)
    root_log_level      = coalesce(var.kafka.root_log_level, "INFO")

    jvm = var.kafka.jvm != null ? merge(var.kafka.jvm,
      tomap({
        xms = contains(keys(var.kafka.jvm), xms) != false ? var.kafka.jvm.xms : "8192m"
        xmx = contains(keys(var.kafka.jvm), xmx) != false ? var.kafka.jvm.xmx : "8192m" }
      )) : {
      xms = "8192m"
      xmx = "8192m"
    }
    resources = var.kafka.resources != null ? merge(var.kafka.resources,
      tomap({
        limit_cpu     = contains(keys(var.kafka.resources), limit_cpu) != false ? var.kafka.resources.limit_cpu : "8"
        limit_memory  = contains(keys(var.kafka.resources), limit_memory) != false ? var.kafka.resources.limit_memory : "63Gi"
        requested_cpu = contains(keys(var.kafka.resources), requested_cpu) != false ? var.kafka.resources.requested_cpu : "6"
        requested_memory = contains(keys(var.kafka.resources), requested_memory) != false ? var.kafka.resources.requested_memory : "60Gi" }
      )) : {
      limit_cpu        = "8"
      limit_memory     = "63Gi"
      requested_cpu    = "6"
      requested_memory = "60Gi"
    }

    kafka_config = var.kafka.kafka_config != null ? merge(var.kafka.kafka_config,
      tomap({
        "auto.create.topics.enable"         = contains(keys(var.kafka.kafka_config), "auto.create.topics.enable") != false ? var.kafka.kafka_config["auto.create.topics.enable"] : "true"
        "controlled.shutdown.enable"        = contains(keys(var.kafka.kafka_config), "controlled.shutdown.enable") != false ? var.kafka.kafka_config["controlled.shutdown.enable"] : "true"
        "default.replication.factor"        = contains(keys(var.kafka.kafka_config), "default.replication.factor") != false ? var.kafka.kafka_config["default.replication.factor"] : "1"
        "delete.topic.enable"               = contains(keys(var.kafka.kafka_config), "delete.topic.enable") != false ? var.kafka.kafka_config["delete.topic.enable"] : "true"
        "inter.broker.protocol.version"     = contains(keys(var.kafka.kafka_config), "inter.broker.protocol.version") != false ? var.kafka.kafka_config["inter.broker.protocol.version"] : "'3.0'"
        "log.flush.scheduler.interval.ms"   = contains(keys(var.kafka.kafka_config), "log.flush.scheduler.interval.ms") != false ? var.kafka.kafka_config["log.flush.scheduler.interval.ms"] : "2000"
        "min.insync.replicas"               = contains(keys(var.kafka.kafka_config), "min.insync.replicas") != false ? var.kafka.kafka_config["min.insync.replicas"] : "1"
        "num.io.threads"                    = contains(keys(var.kafka.kafka_config), "num.io.threads") != false ? var.kafka.kafka_config["num.io.threads"] : "8"
        "num.network.threads"               = contains(keys(var.kafka.kafka_config), "num.network.threads") != false ? var.kafka.kafka_config["num.network.threads"] : "8"
        "num.recovery.threads.per.data.dir" = contains(keys(var.kafka.kafka_config), "num.recovery.threads.per.data.dir") != false ? var.kafka.kafka_config["num.recovery.threads.per.data.dir"] : "1"
        "num.replica.fetchers"              = contains(keys(var.kafka.kafka_config), "num.replica.fetchers") != false ? var.kafka.kafka_config["num.replica.fetchers"] : "4"
        "offsets.topic.replication.factor"  = contains(keys(var.kafka.kafka_config), "offsets.topic.replication.factor") != false ? var.kafka.kafka_config["offsets.topic.replication.factor"] : "3"
        "replica.selector.class"            = contains(keys(var.kafka.kafka_config), "replica.selector.class") != false ? var.kafka.kafka_config["replica.selector.class"] : "org.apache.kafka.common.replica.RackAwareReplicaSelector"
        "socket.receive.buffer.bytes"       = contains(keys(var.kafka.kafka_config), "socket.receive.buffer.bytes") != false ? var.kafka.kafka_config["socket.receive.buffer.bytes"] : "1048576"
        "socket.request.max.bytes"          = contains(keys(var.kafka.kafka_config), "socket.request.max.bytes") != false ? var.kafka.kafka_config["socket.request.max.bytes"] : "104857600"
        "socket.send.buffer.bytes"          = contains(keys(var.kafka.kafka_config), "socket.send.buffer.bytes") != false ? var.kafka.kafka_config["socket.send.buffer.bytes"] : "1048576"
        "transaction.state.log.min.isr"     = contains(keys(var.kafka.kafka_config), "transaction.state.log.min.isr") != false ? var.kafka.kafka_config["transaction.state.log.min.isr"] : "1"
        "transaction.state.log.replication.factor" = contains(keys(var.kafka.kafka_config), "transaction.state.log.replication.factor") != false ? var.kafka.kafka_config["transaction.state.log.replication.factor"] : "3" }
      )) : {
      "auto.create.topics.enable"                = "true"
      "controlled.shutdown.enable"               = "true"
      "default.replication.factor"               = "1"
      "delete.topic.enable"                      = "true"
      "inter.broker.protocol.version"            = "'3.0'"
      "log.flush.scheduler.interval.ms"          = "2000"
      "min.insync.replicas"                      = "1"
      "num.io.threads"                           = "8"
      "num.network.threads"                      = "8"
      "num.recovery.threads.per.data.dir"        = "1"
      "num.replica.fetchers"                     = "4"
      "offsets.topic.replication.factor"         = "3"
      "replica.selector.class"                   = "org.apache.kafka.common.replica.RackAwareReplicaSelector"
      "socket.receive.buffer.bytes"              = "1048576"
      "socket.request.max.bytes"                 = "104857600"
      "socket.send.buffer.bytes"                 = "1048576"
      "transaction.state.log.min.isr"            = "1"
      "transaction.state.log.replication.factor" = "3"
    }
    pod_annotations = var.kafka.pod_annotations != null ? merge(var.kafka.pod_annotations,
      tomap({
        "ad.datadoghq.com/kafka.check_names" = contains(keys(var.kafka.pod_annotations), "ad.datadoghq.com/kafka.check_names") != false ? var.kafka.pod_annotations["ad.datadoghq.com/kafka.check_names"] : "['kafka']"
        "ad.datadoghq.com/kafka.instances"   = contains(keys(var.kafka.pod_annotations), "ad.datadoghq.com/kafka.instances") != false ? var.kafka.pod_annotations["ad.datadoghq.com/kafka.instances"] : "[ 'host' : '%%host%%', 'port' : '9999' ]"
        "ad.datadoghq.com/kafka.logs" = contains(keys(var.kafka.pod_annotations), "ad.datadoghq.com/kafka.logs") != false ? var.kafka.pod_annotations["ad.datadoghq.com/kafka.logs"] : "[ 'source' : 'kafka', 'service' : 'kafka' ]" }
      )) : {
      "ad.datadoghq.com/kafka.check_names" = "['kafka']"
      "ad.datadoghq.com/kafka.instances"   = "[ 'host' : '%%host%%', 'port' : '9999' ]"
      "ad.datadoghq.com/kafka.logs"        = "[ 'source' : 'kafka', 'service' : 'kafka' ]"
    }
    ports = var.kafka.ports != null ? merge(var.kafka.ports,
      tomap({
        "external_port" = contains(keys(var.kafka.ports), "external_port") != false ? var.kafka.ports["external_port"] : 9094
        "internal_port" = contains(keys(var.kafka.ports), "internal_port") != false ? var.kafka.ports["internal_port"] : 9092 }
      )) : {
      "external_port" = 9094
      "internal_port" = 9092
    }
    service_annotations = var.kafka.service_annotations != null ? merge(var.kafka.service_annotations,
      tomap({
        "consul.hashicorp.com/service-sync" = contains(keys(var.kafka.service_annotations), "consul.hashicorp.com/service-sync") != false ? var.kafka.service_annotations["consul.hashicorp.com/service-sync"] : "true" }
      )) : {
      "consul.hashicorp.com/service-sync" = "true"
    }
  }

  kafka_exporter = {
    group_regex            = coalesce(var.kafka_exporter.group_regex, ".*")
    is_sarama_logs_enabled = coalesce(var.kafka_exporter.is_sarama_logs_enabled, true)
    root_log_level         = coalesce(var.kafka_exporter.root_log_level, "debug")
    topic_regex            = coalesce(var.kafka_exporter.topic_regex, ".*")

    resources = var.kafka_exporter.resources != null ? merge(var.kafka_exporter.resources,
      tomap({
        limit_cpu     = contains(keys(var.kafka_exporter.resources), limit_cpu) != false ? var.kafka_exporter.resources.limit_cpu : "1"
        limit_memory  = contains(keys(var.kafka_exporter.resources), limit_memory) != false ? var.kafka_exporter.resources.limit_memory : "512Mi"
        requested_cpu = contains(keys(var.kafka_exporter.resources), requested_cpu) != false ? var.kafka_exporter.resources.requested_cpu : "0.5"
        requested_memory = contains(keys(var.kafka_exporter.resources), requested_memory) != false ? var.kafka_exporter.resources.requested_memory : "256Mi" }
      )) : {
      limit_cpu        = "1"
      limit_memory     = "512Mi"
      requested_cpu    = "0.5"
      requested_memory = "256Mi"
    }

  }

  kowl = {
    atomic           = coalesce(var.kowl.atomic, true)
    chart            = coalesce(var.kowl.chart, "kowl")
    chart_version    = coalesce(var.kowl.chart_version, "2.3.0")
    cleanup_on_fail  = coalesce(var.kowl.cleanup_on_fail, true)
    create_kowl      = coalesce(var.kowl.create_kowl, true)
    create_namespace = coalesce(var.kowl.create_namespace, true)
    name             = coalesce(var.kowl.name, "kowl")
    repository       = coalesce(var.kowl.repository, "https://raw.githubusercontent.com/cloudhut/charts/master/archives")
    root_log_level   = coalesce(var.kowl.root_log_level, "info")
    timeout          = coalesce(var.kowl.timeout, 300)
    wait             = coalesce(var.kowl.wait, true)


    service_annotations = var.kowl.service_annotations != null ? merge(var.kowl.service_annotations,
      tomap({
        "consul.hashicorp.com/service-sync" = contains(keys(var.kowl.service_annotations), "consul.hashicorp.com/service-sync") != false ? var.kowl.service_annotations["consul.hashicorp.com/service-sync"] : "true" }
      )) : {
      "consul.hashicorp.com/service-sync" = "true"
    }
  }

  storage_local_provisioner = {
    atomic                           = coalesce(var.storage_local_provisioner.atomic, true)
    chart                            = coalesce(var.storage_local_provisioner.chart, "vendors/provisioner")
    cleanup_on_fail                  = coalesce(var.storage_local_provisioner.cleanup_on_fail, true)
    create_local_storage_provisioner = coalesce(var.storage_local_provisioner.create_local_storage_provisioner, true)
    create_namespace                 = coalesce(var.storage_local_provisioner.create_namespace, true)
    local_provisioner_hostdir        = coalesce(var.storage_local_provisioner.local_provisioner_hostdir, "/mnt")
    name                             = coalesce(var.storage_local_provisioner.name, "local-storage-provisioner")
    namespace                        = coalesce(var.storage_local_provisioner.namespace, "storage-provisioner")
    storage_class                    = coalesce(var.storage_local_provisioner.storage_class, "nvme-ssd")
    timeout                          = coalesce(var.storage_local_provisioner.timeout, 300)
    wait                             = coalesce(var.storage_local_provisioner.wait, true)


  }

  zookeeper = {
    replicas       = coalesce(var.zookeeper.replicas, 3)
    root_log_level = coalesce(var.zookeeper.root_log_level, "INFO")

    jvm = var.zookeeper.jvm != null ? merge(var.zookeeper.jvm,
      tomap({
        xms = contains(keys(var.zookeeper.jvm), xms) != false ? var.zookeeper.jvm.xms : "4096m"
        xmx = contains(keys(var.zookeeper.jvm), xmx) != false ? var.zookeeper.jvm.xmx : "4096m" }
      )) : {
      xms = "4096m"
      xmx = "4096m"
    }
    resources = var.zookeeper.resources != null ? merge(var.zookeeper.resources,
      tomap({
        limit_cpu     = contains(keys(var.zookeeper.resources), limit_cpu) != false ? var.zookeeper.resources.limit_cpu : "2"
        limit_memory  = contains(keys(var.zookeeper.resources), limit_memory) != false ? var.zookeeper.resources.limit_memory : "6Gi"
        requested_cpu = contains(keys(var.zookeeper.resources), requested_cpu) != false ? var.zookeeper.resources.requested_cpu : "1"
        requested_memory = contains(keys(var.zookeeper.resources), requested_memory) != false ? var.zookeeper.resources.requested_memory : "5Gi" }
      )) : {
      limit_cpu        = "2"
      limit_memory     = "6Gi"
      requested_cpu    = "1"
      requested_memory = "5Gi"
    }

    pod_annotations = var.zookeeper.pod_annotations != null ? merge(var.zookeeper.pod_annotations,
      tomap({
        "ad.datadoghq.com/zookeeper.check_names" = contains(keys(var.zookeeper.pod_annotations), "ad.datadoghq.com/zookeeper.check_names") != false ? var.zookeeper.pod_annotations["ad.datadoghq.com/zookeeper.check_names"] : "['zk']"
        "ad.datadoghq.com/zookeeper.instances"   = contains(keys(var.zookeeper.pod_annotations), "ad.datadoghq.com/zookeeper.instances") != false ? var.zookeeper.pod_annotations["ad.datadoghq.com/zookeeper.instances"] : "[ 'host' : '%%host%%', 'port' : '2181' ]"
        "ad.datadoghq.com/zookeeper.logs" = contains(keys(var.zookeeper.pod_annotations), "ad.datadoghq.com/zookeeper.logs") != false ? var.zookeeper.pod_annotations["ad.datadoghq.com/zookeeper.logs"] : "[ 'source' : 'zookeeper', 'service' : 'zookeeper' ]" }
      )) : {
      "ad.datadoghq.com/zookeeper.check_names" = "['zk']"
      "ad.datadoghq.com/zookeeper.instances"   = "[ 'host' : '%%host%%', 'port' : '2181' ]"
      "ad.datadoghq.com/zookeeper.logs"        = "[ 'source' : 'zookeeper', 'service' : 'zookeeper' ]"
    }
    service_annotations = var.zookeeper.service_annotations != null ? merge(var.zookeeper.service_annotations,
      tomap({
        "consul.hashicorp.com/service-sync" = contains(keys(var.zookeeper.service_annotations), "consul.hashicorp.com/service-sync") != false ? var.zookeeper.service_annotations["consul.hashicorp.com/service-sync"] : "true" }
      )) : {
      "consul.hashicorp.com/service-sync" = "true"
    }
  }
}
